import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters'
import { AlibabaTongyiEmbeddings } from '@langchain/community/embeddings/alibaba_tongyi'
import { Chroma } from '@langchain/community/vectorstores/chroma'
import { ChromaClient } from 'chromadb'
import { Document } from '@langchain/core/documents'
import dotenv from 'dotenv'
import path from 'path'
import { fileURLToPath } from 'url'
import PDFParser from 'pdf2json'
import fs from 'fs'

// åŠ è½½ç¯å¢ƒå˜é‡
dotenv.config()

// è·å–å½“å‰æ–‡ä»¶ç›®å½•
const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)

/**
 * æ¸…ç† PDF è§£æåçš„æ–‡æœ¬æ ¼å¼
 */
function cleanText(text) {
  // ç§»é™¤å•å­—ç¬¦ä¹‹é—´çš„ç©ºæ ¼ï¼ˆé’ˆå¯¹ PDF ç‰¹æ®Šç¼–ç ï¼‰
  let cleaned = text.replace(/(\w)\s+(?=\w)/g, '$1')
  // ç§»é™¤å¤šä½™çš„ç©ºæ ¼
  cleaned = cleaned.replace(/\s+/g, ' ').trim()
  return cleaned
}

/**
 * ä½¿ç”¨ pdf2json åŠ è½½ PDF æ–‡ä»¶
 */
async function loadPDF(pdfPath) {
  return new Promise((resolve, reject) => {
    const pdfParser = new PDFParser()

    pdfParser.on('pdfParser_dataError', (errData) => {
      reject(errData.parserError)
    })

    pdfParser.on('pdfParser_dataReady', (pdfData) => {
      try {
        const pages = pdfData.Pages || []
        const documents = []

        pages.forEach((page, pageIndex) => {
          let pageText = ''
          const texts = page.Texts || []

          texts.forEach((text) => {
            try {
              const decodedText = decodeURIComponent(text.R[0].T)
              pageText += decodedText + ' '
            } catch (e) {
              // å¦‚æœè§£ç å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬
              pageText += text.R[0].T + ' '
            }
          })

          if (pageText.trim()) {
            documents.push(
              new Document({
                pageContent: cleanText(pageText),
                metadata: {
                  source: pdfPath,
                  pageNumber: pageIndex + 1,
                },
              })
            )
          }
        })

        resolve(documents)
      } catch (error) {
        reject(error)
      }
    })

    pdfParser.loadPDF(pdfPath)
  })
}

/**
 * RAG æ£€ç´¢åŠŸèƒ½æ¼”ç¤º
 * 1. åŠ è½½ PDF æ–‡ä»¶
 * 2. æ‹†åˆ†æ–‡æ¡£ä¸º chunks
 * 3. è½¬æ¢ä¸º embedding å‘é‡
 * 4. å­˜å‚¨åˆ° ChromaDB
 * 5. æ‰§è¡Œæ£€ç´¢å¹¶å±•ç¤ºç»“æœ
 */
async function ragDemo() {
  try {
    console.log('=== RAG æ£€ç´¢åŠŸèƒ½æ¼”ç¤º ===\n')

    // 1. åŠ è½½ PDF æ–‡ä»¶
    console.log('ğŸ“„ æ­¥éª¤ 1: åŠ è½½ PDF æ–‡ä»¶...')
    const pdfPath = path.join(__dirname, '../files/nike-inc-2025.pdf')
    const docs = await loadPDF(pdfPath)
    console.log(`âœ… æˆåŠŸåŠ è½½ PDFï¼Œå…± ${docs.length} é¡µ\n`)

    // 2. æ‹†åˆ†æ–‡æ¡£ä¸º chunks
    console.log('âœ‚ï¸  æ­¥éª¤ 2: æ‹†åˆ†æ–‡æ¡£ä¸º chunks...')
    const textSplitter = new RecursiveCharacterTextSplitter({
      chunkSize: 1000,
      chunkOverlap: 200,
    })
    const splitDocs = await textSplitter.splitDocuments(docs)
    console.log(`âœ… æˆåŠŸæ‹†åˆ†ä¸º ${splitDocs.length} ä¸ª chunks`)

    // ä¸ºäº†æ¼”ç¤ºï¼Œåªä½¿ç”¨å‰ 100 ä¸ª chunks ä»¥é¿å…é…é¢é™åˆ¶
    const limitedDocs = splitDocs.slice(0, 100)
    console.log(
      `ğŸ“ ä¸ºé¿å…é…é¢é™åˆ¶ï¼Œæœ¬æ¬¡æ¼”ç¤ºä½¿ç”¨å‰ ${limitedDocs.length} ä¸ª chunks\n`
    )

    // 3. åˆå§‹åŒ– Embedding æ¨¡å‹
    console.log('ğŸ”¢ æ­¥éª¤ 3: åˆå§‹åŒ– Embedding æ¨¡å‹...')
    const embeddings = new AlibabaTongyiEmbeddings({
      apiKey: process.env.ALIBABA_API_KEY,
      batchSize: 10, // é™ä½æ‰¹å¤„ç†å¤§å°ä»¥é¿å…é…é¢é™åˆ¶
    })
    console.log('âœ… Embedding æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ\n')

    // 4. å­˜å‚¨åˆ° ChromaDB
    console.log('ğŸ’¾ æ­¥éª¤ 4: å­˜å‚¨åˆ° ChromaDB...')
    const collectionName = 'nike_inc_2025'

    // é¦–å…ˆæ¸…ç†å·²å­˜åœ¨çš„é›†åˆ
    try {
      const chromaClient = new ChromaClient({
        path: 'http://localhost:8000',
      })
      await chromaClient.deleteCollection({ name: collectionName })
      console.log('ğŸ—‘ï¸  å·²åˆ é™¤æ—§çš„é›†åˆ')
    } catch (error) {
      // é›†åˆä¸å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯
    }

    // åˆ›å»ºå‘é‡å­˜å‚¨
    const vectorStore = await Chroma.fromDocuments(limitedDocs, embeddings, {
      collectionName: collectionName,
      url: 'http://localhost:8000',
    })
    console.log(`âœ… æˆåŠŸå­˜å‚¨ ${limitedDocs.length} ä¸ª chunks åˆ° ChromaDB\n`)

    // 5. æ‰§è¡Œæ£€ç´¢æ¼”ç¤º
    console.log('ğŸ” æ­¥éª¤ 5: æ‰§è¡Œæ£€ç´¢æ¼”ç¤º...\n')
    console.log('='.repeat(80))

    // æ£€ç´¢ç¤ºä¾‹ 1: å…³äº Nike çš„æ”¶å…¥
    console.log('\nã€æ£€ç´¢ç¤ºä¾‹ 1ã€‘')
    const query1 = "What was Nike's revenue in 2025?"
    console.log(`æŸ¥è¯¢é—®é¢˜: ${query1}`)
    console.log('-'.repeat(80))

    const results1 = await vectorStore.similaritySearchWithScore(query1, 3)
    console.log(`æ‰¾åˆ° ${results1.length} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ:\n`)

    results1.forEach(([doc, score], index) => {
      console.log(`ç»“æœ ${index + 1}: (ç›¸ä¼¼åº¦: ${(score * 100).toFixed(2)}%)`)
      console.log(`å†…å®¹: ${doc.pageContent.substring(0, 400)}...`)
      console.log(`æ¥æº: ç¬¬ ${doc.metadata.pageNumber || 'æœªçŸ¥'} é¡µ`)
      console.log('-'.repeat(80))
    })

    // æ£€ç´¢ç¤ºä¾‹ 2: å…³äº Nike çš„äº§å“
    console.log('\nã€æ£€ç´¢ç¤ºä¾‹ 2ã€‘')
    const query2 = "What are Nike's main product categories?"
    console.log(`æŸ¥è¯¢é—®é¢˜: ${query2}`)
    console.log('-'.repeat(80))

    const results2 = await vectorStore.similaritySearchWithScore(query2, 3)
    console.log(`æ‰¾åˆ° ${results2.length} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ:\n`)

    results2.forEach(([doc, score], index) => {
      console.log(`ç»“æœ ${index + 1}: (ç›¸ä¼¼åº¦: ${(score * 100).toFixed(2)}%)`)
      console.log(`å†…å®¹: ${doc.pageContent.substring(0, 400)}...`)
      console.log(`æ¥æº: ç¬¬ ${doc.metadata.pageNumber || 'æœªçŸ¥'} é¡µ`)
      console.log('-'.repeat(80))
    })

    console.log('\nâœ¨ RAG æ£€ç´¢æ¼”ç¤ºå®Œæˆï¼')
  } catch (error) {
    console.error('âŒ é”™è¯¯:', error.message)
    console.error(error)
  }
}

// è¿è¡Œæ¼”ç¤º
ragDemo()